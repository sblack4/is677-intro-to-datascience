---
title: "Homework10"
author: "Steven Black"
date: "4/24/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## IS 677: Introduction to Data Science Spring 2019 
Homework Assignment 10  (Due: April 28, 2019, midnight EST) 
 
Use the CIFAR10 dataset that comes with Keras. It has 50,000 training and 10,000 testing images of 10 classes: 

- airplane
- automobile
- bird
- cat
- deer
- dog
- frog
- horse
- ship
- truck 

The image size is 32\*32*3. The 3 indicates that these are color images. 

```{r}
library(keras)
k_clear_session()
```

 
1.	Clear the session and load the CIFAR10 data into a variable called cifar. (5 points)
```{r}
cifar <- dataset_cifar10()
```


2.	Create a small training dataset using the first 1000 training images (and the corresponding labels) from CIFAR10. Similarly, create a small test dataset (and the corresponding labels) using the first test 500 images from CIFAR10. (5 points).
```{r}
num_training_images <- 1000
num_testing_images <- 500

train_images <- cifar$train$x[1:num_training_images, 1:32, 1:32, 1:3]
train_labels <- cifar$train$y[1:num_training_images]

test_images <- cifar$test$x[1:num_testing_images, 1:32, 1:32, 1:3]
test_labels <- cifar$test$y[1:num_testing_images]
```


3.	Create one-hot encoding for the labels for both train and test labels. (5 points)
```{r}
train_labels_categorical <- to_categorical(train_labels)
test_labels_categorical <- to_categorical(test_labels)
```


4.	Instantiate a VGG16 convolutional base without the top layer. (5 points)
```{r}
conv_base <- application_vgg16(  
  weights = "imagenet",  
  include_top = FALSE,  
  input_shape = c(32, 32, 3)
)
conv_base
```


5.	Extract features from the CIFAR10 images so as to fit the conv_base. (40 points)
```{r}
datagen <- image_data_generator(rescale = 1/255)
batch_size <- 10

train_generator <- flow_images_from_data(    
	x = train_images,    
	y = train_labels_categorical,
	generator = datagen,    
	batch_size = batch_size
)
features_batch <- conv_base %>% predict_generator(train_generator, 
                                                  steps = num_training_images / batch_size) 
 
test_generator <- flow_images_from_data(    
	x = test_images,    
	y = test_labels_categorical,
	generator = datagen,    
	batch_size = batch_size
)

test_features_batch <- conv_base %>% predict_generator(test_generator, 
                                                  steps = num_testing_images / batch_size) 
```

```{r}
dim(features_batch)
```



6.	Flatten the features in order to feed them to a densely connected classifier. (5 points)
```{r}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 1 * 1 * 512))
}

train_features <- reshape_features(features_batch)
test_features <- reshape_features(test_features_batch)
```


7.	Build a model with one dense layer with 256 units and “relu” activation, one dropout layer with 50% dropout rate, and a dense output layer with appropriate parameters. (15 points)
```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu",
              input_shape = 1 * 1 * 512) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = "sigmoid")

model
```

8.	Compile the model with categorical_crossentropy as the loss function and optimizer_rmsprop with 0.01% learning rate (lr=0.0001). (5 points)
```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr=0.0001),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

9.	Fit the model using 30 epochs. Plot the loss and accuracies. (5 points)
```{r}
history <- model %>% fit(
  train_features, train_labels_categorical,
  epochs = 30,
  batch_size = 20,
  validation_data = list(test_features, test_labels_categorical)
)

plot(history)
```

10.	Note that the model is likely to have low accuracy. Explain why. (10 points)

This model is overfitting almost from the start. The accuracy for the training set is drastically higher than that of the validation set. The overfitting is probably due to not using data augmentation, which is important for a small dataset like this.  



